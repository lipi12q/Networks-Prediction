{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Graph Mining\n",
    "*Due Wednesday November 14th, 2018 at 11:59 pm*\n",
    "\n",
    "*Notebook Author: Koki Sasagawa*\n",
    "\n",
    "Mining a large social network to uncover how well homophily can predict identity as well as the network structure. \n",
    "\n",
    "**Task2:** Attribute Prediction - most of the nodes in the social network are provided with one or more attributes that can be drawn from different types. (e.g., age, occupation, musical preference, etc. ) Predict the probabilities of attributes for a set of completely unlabeled nodes\n",
    "\n",
    "## Data\n",
    "\n",
    "1. `labeled-vertices.train.tsv` & `labeled-vertices.dev.tsv` & `unlabeled-verticies.test.tsv` \n",
    "   - users with attributes formatted as the following: \n",
    "\n",
    "   > - **vertex1** T1:3 T7:1 T4:2\n",
    "   > - **vertex2** T2:4\n",
    "   > - **vertex3** T4:3 T3:1\n",
    "   \n",
    "   - Each value is specified as `AttributeType:Value`\n",
    "   - Not every user will have their attributes listed \n",
    "   - Majority users should have at least 2 attribute set\n",
    "2. `unlabeled-verticies.test.txt` - simply have list of vertices that should predict attributes and their values \n",
    "\n",
    "## Submission \n",
    "\n",
    "**Attribute prediction** should be a csv file with two columns: id and attr. \n",
    "The attr column should contain a space-deliminted list of the attributes you think the user with that id has. The file should have the following structure:\n",
    "\n",
    "> id, attr\n",
    "> \n",
    "> 123, T0:0 T1:1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "# import os\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network graph...\n",
      "Network graph created. Process took 255.3354 seconds\n",
      "Number of edges: 30915267\n",
      "Number of nodes: 6626753\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating network graph...\")\n",
    "start_time = time.time() \n",
    "\n",
    "with open(\"../data/network.tsv\", 'rb') as f:\n",
    "    grph = nx.read_edgelist(path=f, delimiter='\\t', encoding='utf8')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Network graph created. Process took {:.04f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Check that graph is of correct size\n",
    "print(\"Number of edges: {}\".format(grph.number_of_edges())) # There should be 30915267\n",
    "print(\"Number of nodes: {}\".format(grph.number_of_nodes())) # There should be 6626753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load all files for attribute prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Reading in dev set...\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# dev_set_chunks = pd.read_csv(\"../data2/labeled-vertices.dev.tsv\",\n",
    "#                       delimiter='\\t',\n",
    "#                       usecols=[0,1],\n",
    "#                       names=['id', 'attr'],\n",
    "#                       header=None,\n",
    "#                       chunksize=100000)\n",
    "\n",
    "# dev_set = pd.concat(dev_set_chunks)\n",
    "\n",
    "# # Free memory \n",
    "# dev_set_chunks = None\n",
    "\n",
    "# # with open('../data2/labeled-vertices.dev.test.tsv') as f:\n",
    "# #     for line in f:\n",
    "# #         dev_set.append(line.rstrip())\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"Dev set loaded. Process took {:.04f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# # Check \n",
    "# dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in training set...\n",
      "Train set loaded. Process took 2.0877 seconds\n",
      "5301403\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5509623</td>\n",
       "      <td>T0:0 T1:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6334893</td>\n",
       "      <td>T0:0 T1:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1218900</td>\n",
       "      <td>T0:1 T1:2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3871398</td>\n",
       "      <td>T0:1 T1:2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3942361</td>\n",
       "      <td>T0:0 T1:3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       attr\n",
       "0  5509623  T0:0 T1:0\n",
       "1  6334893  T0:0 T1:1\n",
       "2  1218900  T0:1 T1:2\n",
       "3  3871398  T0:1 T1:2\n",
       "4  3942361  T0:0 T1:3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading in training set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_set_chunks = pd.read_csv(\"../data2/labeled-vertices.train.tsv\",\n",
    "                      delimiter='\\t',\n",
    "                      usecols=[0,1],\n",
    "                      names=['id', 'attr'],\n",
    "                      header=None,\n",
    "                      chunksize=100000)\n",
    "\n",
    "train_set = pd.concat(train_set_chunks)\n",
    "\n",
    "# Free memory \n",
    "train_set_chunks = None\n",
    "\n",
    "# with open('../data2/labeled-vertices.dev.test.tsv') as f:\n",
    "#     for line in f:\n",
    "#         dev_set.append(line.rstrip())\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Train set loaded. Process took {:.04f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Check \n",
    "print(train_set.shape[0])\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in test set...\n",
      "Test set loaded. Process took 0.2285 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['4546232', '3711008', '6394112', '5883774', '2843733']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading in test set...')\n",
    "start_time = time.time()\n",
    "\n",
    "test_set = []\n",
    "\n",
    "with open('../data2/unlabeled-vertices.test.txt') as f:\n",
    "    for line in f:\n",
    "        test_set.append(line.rstrip())\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Test set loaded. Process took {:.04f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Check \n",
    "test_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions for attribute prediction\n",
    "\n",
    "The following function will compute some similarity metric on the neighbors of a node, and the node will inherit the attributes of the highest scoring node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_jaccard_similarity(test_set, grph):\n",
    "    '''Calculate then jaccard similarity of neighbors and return the most similar node\n",
    "    \n",
    "    If jaccard similarity of two nodes is zero, return 0 \n",
    "    \n",
    "    :params test_set: target nodes we want to find the most similar neighbor for\n",
    "    :type test_set: list\n",
    "    :params grph: network containing target nodes\n",
    "    :type grph: networkx graph\n",
    "    :return: node and the most similar neighboring node\n",
    "    :rtype: dict\n",
    "    '''\n",
    "    \n",
    "    # Store results\n",
    "    sim_results = {}\n",
    "    \n",
    "    for i in test_set: \n",
    "        nearest_neighbors = list(grph.neighbors(i))\n",
    "        # If a node only has only 1 neighbor, inherit that neighbors attributes \n",
    "        if len(nearest_neighbors) == 1: \n",
    "            sim_results[i] = nearest_neighbors[0]\n",
    "        else:\n",
    "            # Generate node-neighbor pairings \n",
    "            node_pairs = [(i, j) for j in nearest_neighbors]\n",
    "            preds = nx.jaccard_coefficient(grph, ebunch=node_pairs)\n",
    "\n",
    "            # Sort by similarity score\n",
    "            preds = list(preds)\n",
    "            preds.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "            # Take the node with highest sim score\n",
    "            # If similarity is 0, return 0\n",
    "            if preds[0][2] == 0.0:\n",
    "                sim_results[i] = 0\n",
    "            else:\n",
    "                sim_results[i] = preds[0][1]\n",
    "\n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_adamic_adar_similarity(test_set, grph):\n",
    "    '''Calculate then adamic/adar similarity of neighbors and return the most similar node\n",
    "    \n",
    "    If adamic/adar similarity of two nodes is zero, return 0\n",
    "    \n",
    "    :params test_set: target nodes we want to find the most similar neighbor for\n",
    "    :type test_set: list \n",
    "    :params grph: network containing target nodes\n",
    "    :type grph: networkx graph\n",
    "    :return: node and the most similar neighboring node\n",
    "    :rtype: dict\n",
    "    '''\n",
    "    \n",
    "    # Store results\n",
    "    sim_results = {}\n",
    "    \n",
    "    for i in test_set:\n",
    "        nearest_neighbors = list(grph.neighbors(i))\n",
    "        # If a node only has only 1 neighbor, inherit that neighbors attributes \n",
    "        if len(nearest_neighbors) == 1: \n",
    "            sim_results[i] = nearest_neighbors[0]\n",
    "        else:\n",
    "            # Generate node-neighbor pairings \n",
    "            node_pairs = [(i, j) for j in nearest_neighbors]\n",
    "            preds = nx.adamic_adar_index(grph, ebunch=node_pairs)\n",
    "\n",
    "            # Sort by similarity score\n",
    "            preds = list(preds)\n",
    "            preds.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "            # Take the node with highest sim score\n",
    "            # If similarity is 0, return 0\n",
    "            if preds[0][2] == 0.0:\n",
    "                sim_results[i] = 0\n",
    "            else:\n",
    "                sim_results[i] = preds[0][1]\n",
    "\n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_preferential_attachment_score(test_set, grph):\n",
    "    '''Calculate then preferential attachment scores of neighbors and return the highest scoring node\n",
    "    \n",
    "    :params test_set: target nodes we want to find the most similar neighbor for\n",
    "    :type test_set: list\n",
    "    :params grph: network containing target nodes\n",
    "    :type grph: networkx graph\n",
    "    :return: node and the most similar neighboring node\n",
    "    :rtype: dict\n",
    "    '''\n",
    "    \n",
    "    # Store results\n",
    "    sim_results = {}\n",
    "    \n",
    "    for i in test_set:\n",
    "        nearest_neighbors = list(grph.neighbors(i))\n",
    "        # If a node only has only 1 neighbor, inherit that neighbors attributes \n",
    "        if len(nearest_neighbors) == 1: \n",
    "            sim_results[i] = nearest_neighbors[0]\n",
    "        else:\n",
    "            # Generate node-neighbor pairings \n",
    "            node_pairs = [(i, j) for j in nearest_neighbors]\n",
    "            preds = nx.preferential_attachment(grph, ebunch=node_pairs)\n",
    "\n",
    "            # Sort by score\n",
    "            preds = list(preds)\n",
    "            preds.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "            # Take the node with highest score\n",
    "            # If score is 0, return 0\n",
    "            if preds[0][2] == 0.0:\n",
    "                sim_results[i] = 0\n",
    "            else:\n",
    "                sim_results[i] = preds[0][1]\n",
    "\n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate node pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity and Adamic/adar similarity both require nodes to have common neighbors, or the similarity score will be zero. Preferential attachment, however, can still be calculated as it is based on the idea that nodes will attach to nodes of higher degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running jaccard similarity...\n",
      "Node pairs generated. Process took 18.2419 seconds\n"
     ]
    }
   ],
   "source": [
    "# print('Running jaccard similarity...')\n",
    "# start_time = time.time()\n",
    "\n",
    "# jaccard_similarity_nodes = highest_jaccard_similarity(test_set, grph)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"Node pairs generated. Process took {:.04f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adamic/adar similarity...\n",
      "Node pairs generated. Process took 8.6248 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Running adamic/adar similarity...')\n",
    "start_time = time.time()\n",
    "\n",
    "adamic_adar_nodes = highest_adamic_adar_similarity(test_set, grph)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Node pairs generated. Process took {:.04f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running preferential attachment...\n",
      "Node pairs generated. Process took 2.4117 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Running preferential attachment...')\n",
    "start_time = time.time()\n",
    "\n",
    "preferential_attachment_nodes = highest_preferential_attachment_score(test_set, grph)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Node pairs generated. Process took {:.04f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions with jaccard\n",
    "# predictions = pd.DataFrame({\n",
    "#                         'id': test_set,\n",
    "#                         'jaccard_similarity': list(jaccard_similarity_nodes.values()), \n",
    "#                         'preferential_attachment': list(preferential_attachment_nodes.values()),\n",
    "#                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with Adamic/Adar\n",
    "predictions = pd.DataFrame({\n",
    "                        'id': test_set,\n",
    "                        'adamic_adar': list(adamic_adar_nodes.values()), \n",
    "                        'preferential_attachment': list(preferential_attachment_nodes.values()),\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adamic_adar</th>\n",
       "      <th>preferential_attachment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4546232</td>\n",
       "      <td>2494614</td>\n",
       "      <td>2494614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3711008</td>\n",
       "      <td>2444912</td>\n",
       "      <td>2174169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6394112</td>\n",
       "      <td>6223074</td>\n",
       "      <td>6223074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5883774</td>\n",
       "      <td>4485305</td>\n",
       "      <td>4485305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2843733</td>\n",
       "      <td>3931905</td>\n",
       "      <td>3931905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id adamic_adar preferential_attachment\n",
       "0  4546232     2494614                 2494614\n",
       "1  3711008     2444912                 2174169\n",
       "2  6394112     6223074                 6223074\n",
       "3  5883774     4485305                 4485305\n",
       "4  2843733     3931905                 3931905"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Predictions from jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's: 58932\n"
     ]
    }
   ],
   "source": [
    "# Find the number of cells where jaccard similarity did not return a similarity score and returned 0\n",
    "print(\"Number of 0\\'s: {}\".format(predictions.shape[0] - np.count_nonzero(predictions['jaccard_similarity'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every row where jaccard similarity failed to return a candidate node due to lack of common neighbors,\n",
    "# use the node returned by preferential attachment\n",
    "predictions['attr'] = np.where(predictions['jaccard_similarity'] == 0, predictions['preferential_attachment'], predictions['jaccard_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of 0\\'s: {}\".format(predictions.shape[0] - np.count_nonzero(predictions['attr'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need columns jaccard_similarity and preferential attachment, so drop them\n",
    "predictions.drop(['jaccard_similarity', 'preferential_attachment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Predictions from adamic adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's: 58932\n"
     ]
    }
   ],
   "source": [
    "# Find the number of cells where jaccard similarity did not return a similarity score and returned 0\n",
    "print(\"Number of 0\\'s: {}\".format(predictions.shape[0] - np.count_nonzero(predictions['adamic_adar'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every row where jaccard similarity failed to return a candidate node due to lack of common neighbors,\n",
    "# use the node returned by preferential attachment\n",
    "predictions['attr'] = np.where(predictions['adamic_adar'] == 0, predictions['preferential_attachment'], predictions['adamic_adar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of 0\\'s: {}\".format(predictions.shape[0] - np.count_nonzero(predictions['attr'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need columns jaccard_similarity and preferential attachment, so drop them\n",
    "predictions.drop(['adamic_adar', 'preferential_attachment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain attributes for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4546232</td>\n",
       "      <td>2494614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3711008</td>\n",
       "      <td>2444912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6394112</td>\n",
       "      <td>6223074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5883774</td>\n",
       "      <td>4485305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2843733</td>\n",
       "      <td>3931905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     attr\n",
       "0  4546232  2494614\n",
       "1  3711008  2444912\n",
       "2  6394112  6223074\n",
       "3  5883774  4485305\n",
       "4  2843733  3931905"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# Variable type in train_set \n",
    "print(type(train_set['id'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# The values in the prediction dataframe are currently str type\n",
    "# convert them to int64 to allow merge with train_set\n",
    "print(type(predictions['attr'][0]))\n",
    "\n",
    "predictions = predictions.astype(dtype=np.int64, copy=True)\n",
    "print(type(predictions['attr'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>attr_x</th>\n",
       "      <th>id_y</th>\n",
       "      <th>attr_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4546232</td>\n",
       "      <td>2494614</td>\n",
       "      <td>2494614.0</td>\n",
       "      <td>T0:0 T1:1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3711008</td>\n",
       "      <td>2444912</td>\n",
       "      <td>2444912.0</td>\n",
       "      <td>T0:0 T1:1762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6394112</td>\n",
       "      <td>6223074</td>\n",
       "      <td>6223074.0</td>\n",
       "      <td>T0:0 T1:1914 T8:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5883774</td>\n",
       "      <td>4485305</td>\n",
       "      <td>4485305.0</td>\n",
       "      <td>T0:0 T1:944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2843733</td>\n",
       "      <td>3931905</td>\n",
       "      <td>3931905.0</td>\n",
       "      <td>T0:0 T1:538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_x   attr_x       id_y             attr_y\n",
       "0  4546232  2494614  2494614.0       T0:0 T1:1766\n",
       "1  3711008  2444912  2444912.0       T0:0 T1:1762\n",
       "2  6394112  6223074  6223074.0  T0:0 T1:1914 T8:0\n",
       "3  5883774  4485305  4485305.0        T0:0 T1:944\n",
       "4  2843733  3931905  3931905.0        T0:0 T1:538"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge preditions dataframe with trainset by id \n",
    "results = predictions.merge(train_set, left_on='attr', right_on='id', how='left')\n",
    "# Check results\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "results.drop(['attr_x', 'id_y'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4546232</td>\n",
       "      <td>T0:0 T1:1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3711008</td>\n",
       "      <td>T0:0 T1:1762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6394112</td>\n",
       "      <td>T0:0 T1:1914 T8:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5883774</td>\n",
       "      <td>T0:0 T1:944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2843733</td>\n",
       "      <td>T0:0 T1:538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               attr\n",
       "0  4546232       T0:0 T1:1766\n",
       "1  3711008       T0:0 T1:1762\n",
       "2  6394112  T0:0 T1:1914 T8:0\n",
       "3  5883774        T0:0 T1:944\n",
       "4  2843733        T0:0 T1:538"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "results.columns = ['id', 'attr']\n",
    "\n",
    "# Show results\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results contain 662675 nodes\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print('Results contain {} nodes'.format(results.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "results.to_csv('../data2/attribute_predictions_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
